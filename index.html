<!DOCTYPE html>
<html lang="en">

<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, shrink-to-fit=no, initial-scale=1">
    
    <meta name="author" content="Lexionbear: https://github.com/lexionbear">

    <title>Machine Learning Models</title>
	<meta name="description" content="Collections of state-of-art tensorflow machine learning algorithms and models">
	
    <!-- Bootstrap Core CSS -->
    <link href="css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom CSS -->
    <link href="css/simple-sidebar.css" rel="stylesheet">

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

</head>

<body>
    <div id="wrapper">

        <!-- Sidebar -->
        <div id="sidebar-wrapper">
            <ul class="sidebar-nav">
                <li class="sidebar-brand">
                    <a href="">
                        Models
                    </a>
                </li>
                <li>
                    <a href="#googlenet">GoogleNet Image Classifier</a>
                </li>
				<li>
                    <a href="#resnet">ResNet Image Classifier</a>
                </li>
				<li>
                    <a href="#vgg16">VGG16 Image Classifier</a>
                </li>
				<li>
                    <a href="#imagetalk">Image Captioning</a>
                </li>
				<li>
                    <a href="#textrank">TextRank - Text Summarization</a>
                </li>
				<li>
                    <a href="#syntaxnet">SyntaxNet - POS Tagging</a>
                </li>
				<li>
                    <a href="#gan">GAN - Generative Model</a>
                </li>
				<li>
                    <a href="#vae">Variational Autoencoder</a>
                </li>
            </ul>
        </div>
        <!-- /#sidebar-wrapper -->

        <!-- Page Content -->
        <div id="page-content-wrapper">
            <div class="container-fluid row">
				<div class="col-md-1"></div>
				<div class="col-md-10">
					<div id="googlenet" class="row">
						<div class="col-lg-12 row">
							<h1>GoogleNet Image Classifier</h1>
							<p class="lead">Publication: <a href="http://arxiv.org/abs/1409.4842">Going Deeper with Convolution</a></p>
							<p>GoogleNet is a cheap and relatively accurate 1000 class image classifier first published in 2014</p>
							<div class="col-lg-8">
								<table class="table table-bordered table-hover table-responsive"> 
									<tbody> 
										<tr> 
											<th scope="row" rowspan="2">Accuracy</th>
											<td>ILSVRC 2014 dataset: Accuracy (Top-5) <b>93.33%</b></td>
										</tr>
										<tr>
											<td>ILSVRC 2012 dataset: Accuracy (Top-5) <b>89.06%</b></td>
										</tr>
										<tr> 
											<th scope="row">Model Size</th>
											<td>27Mb in npy format</td>
										</tr> 
										<tr> 
											<th scope="row">Architecture</th>
											<td>
												<p>9 identical blocks of parallel convolutional neural network layers with downsampling, shown below</p>
												<img src="img/googlenet/googlenet_block.png" class="img-responsive" alt="GoogleNet basic block"/>
											</td>
										</tr>
										<tr> 
											<th scope="row">Advantage</th>
											<td>
												<ul>
													<li>Simplicity: the network consists of 9 identical and relatively simple blocks</li>
													<li>Parallelism: the network layers within each block are structure in 4 parallel pathway</li>
													<li>Computation and memory efficiency: because of the parallel network implementation and the dimension reduction layers in each block, the model size is contained within 27Mb npy file, and its execution time beats VGG or ResNet on commodity hardware.</li>
												</ul>
											</td>
										</tr>
										<tr> 
											<th scope="row">Disadvantage</th>
											<td>
												<ul>
													<li>Lower accuracy: the high efficiency comes at a small cost of the model accuracy on ILSVRC 2012</li>
												</ul>
											</td>
										</tr>
									</tbody>
								</table>
							</div>
							<div class="col-lg-4"></div>
						</div>
						
						<div class="col-lg-12 row">
							<p>Model code in Tensorflow: <a href="https://lexiondebug.blob.core.windows.net/mlmodel/models/googlenet.py">GoogleNet Code</a></p>
							<p>Pre-trained model in npy format: <a href="https://lexiondebug.blob.core.windows.net/mlmodel/models/googlenet.npy">GoogleNet Model</a></p>
							<p>Output label lookup dictionary: <a href="https://lexiondebug.blob.core.windows.net/mlmodel/models/imagenet-classes.txt">Imagenet Classes</a></p>
							<p>The model is converted into Tensorflow using ethereon's <code><a href="https://github.com/ethereon/caffe-tensorflow">caffe-tensorflow</a></code> library. The converted network requires the library to initialize network structure.</p>
						</div>
						<hr/>
					</div>
					<div id="resnet" class="row">
						<div class="col-lg-12 row">
							<h1>ResNet50 Image Classifier</h1>
							<p class="lead">Publication: <a href="http://arxiv.org/abs/1512.03385">Deep Residual Learning for Image Recognition</a></p>
							<p>ResNet50 is a highly accurate model published by Microsoft research. It's gain in accuracy comes at a cost of computational expenses. Both its model memory cost and execution time exceed those of GoogleNet.</p>
							<div class="col-lg-8">
								<table class="table table-bordered table-hover table-responsive"> 
									<tbody> 
										<tr> 
											<th scope="row">Accuracy</th>
											<td>ILSVRC 2012 dataset: Accuracy (Top-5) <b>92.02%</b></td>
										</tr> 
										<tr> 
											<th scope="row">Model Size</th>
											<td>100Mb in npy format</td>
										</tr> 
										<tr> 
											<th scope="row">Architecture</th>
											<td>
												<p>50 layers of similar blocks with "bypass connections" shown as the <i>x identity</i> below</p>
												<img src="img/resnet/resnet_block.png" class="img-responsive" alt="GoogleNet basic block"/>
												<p>Shortcut path serves as a model simplifier and provides the benefit of simple models in a complex network. If shortcut path is dominant, the layers between this shortcut are essentially ignored, reducing the complexity of the model in effect.</p>
											</td>
										</tr>
										<tr> 
											<th scope="row">Advantage</th>
											<td>
												<ul>
													<li>High accuracy: ResNet achieves one of the best performance accuracy, beating VGG and GoogleNet in ILSVRC 2012 testset</li>
												</ul>
											</td>
										</tr>
										<tr> 
											<th scope="row">Disadvantage</th>
											<td>
												<ul>
													<li>Relative complex model: although simple in concept, ResNet implementation is highly complicated due to the extensive use of shortcut path that skips layers and pooling, normalizations operations. This increases debugging and innovation cost.</li>
												</ul>
											</td>
										</tr>
									</tbody>
								</table>
							</div>
							<div class="col-lg-4"></div>
						</div>
						<div class="col-lg-12 row">
							<p>Model code in Tensorflow: <a href="https://lexiondebug.blob.core.windows.net/mlmodel/models/resnet.py">ResNet Code</a></p>
							<p>Pre-trained model in npy format: <a href="https://lexiondebug.blob.core.windows.net/mlmodel/models/res50.npy">ResNet Model</a></p>
							<p>Output label lookup dictionary: <a href="https://lexiondebug.blob.core.windows.net/mlmodel/models/imagenet-classes.txt">Imagenet Classes</a></p>
							<p>The model is converted into Tensorflow using ethereon's <code><a href="https://github.com/ethereon/caffe-tensorflow">caffe-tensorflow</a></code> library. The converted network requires the library to initialize network structure.</p>
						</div>
						<hr/>
					</div>	
					<div id="vgg16" class="row">
						<div class="col-lg-12 row">
							<h1>VGG16 Image Classifier</h1>
							<p class="lead">Publication: <a href="http://arxiv.org/abs/1409.1556">Very Deep Convolutional Networks for Large-Scale Image Recognition</a></p>
							<p>VGG is published by researchers at University of Oxford. The highlight is its simplicity in architecture. Majority of the network consists of convolution layers and dropout layers in simple cascading fashion.</p>
							<div class="col-lg-8">
								<table class="table table-bordered table-hover table-responsive"> 
									<tbody> 
										<tr> 
											<th scope="row">Accuracy</th>
											<td>ILSVRC 2012 dataset: Accuracy (Top-5) <b>89.88%</b></td>
										</tr> 
										<tr> 
											<th scope="row">Model Size</th>
											<td>540Mb in npy format</td>
										</tr> 
										<tr> 
											<th scope="row">Architecture</th>
											<td>
												<ul>
													<li>As the name suggests, VGG16 consists of 16 layers. Several variation exists. It repeats the pattern of 2 convolution layers followed by 1 dropout layers until the fully connected layer at the end.</li>
													<li>Its design follows the philosophy of conserving time complexity, where each time max pooling reduce the input dimension by 2, number of kernals in the next convolutional layer increases by 2</li>
												</ul>
											</td>
										</tr>
										<tr> 
											<th scope="row">Advantage</th>
											<td>
												<ul>
													<li>Simplicity: majority of the network consists of convolution layers and dropout layers in simple cascading fashion. It has no shortcut, normalization or concatenations operations.</li>
												</ul>
											</td>
										</tr>
										<tr> 
											<th scope="row">Disadvantage</th>
											<td>
												<ul>
													<li>High memory and computational cost: although it is simple in network architecture, the exponential increase of convolutional kernals lead to significant increase of its model size as well as model execution time, comparing with GoogleNet and ResNet</li>
												</ul>
											</td>
										</tr>
									</tbody>
								</table>
							</div>
							<div class="col-lg-4"></div>
						</div>
						<div class="col-lg-12 row">
							<p>Model code in Tensorflow: <a href="https://lexiondebug.blob.core.windows.net/mlmodel/models/vgg.py">VGG16 Code</a></p>
							<p>Pre-trained model in npy format: <a href="https://lexiondebug.blob.core.windows.net/mlmodel/models/VGG_16.npy">VGG16 Model <b>(540Mb)</b></a></p>
							<p>Output label lookup dictionary: <a href="https://lexiondebug.blob.core.windows.net/mlmodel/models/imagenet-classes.txt">Imagenet Classes</a></p>
							<p>The model is converted into Tensorflow using ethereon's <code><a href="https://github.com/ethereon/caffe-tensorflow">caffe-tensorflow</a></code> library. The converted network requires the library to initialize network structure.</p>
						</div>
						<hr/>
					</div>	
					<div id="imagetalk" class="row">
						<div class="col-lg-12 row">
							<h1>Image Captioning: Show, (Attend) and Tell</h1>
							<p class="lead">Publication: <a href="https://arxiv.org/abs/1411.4555">Show and Tell: A Neural Image Caption Generator</a></p>
							<p class="lead">Publication: <a href="https://arxiv.org/abs/1502.03044">Show, Attend and Tell: Neural Image Caption Generation with Visual Attention</a></p>
							<p>Google's original "Show and Tell" network builds a LSTM recurrent network on top of GoogleNet Image classifier to generate captions from images. The CNN googlenet interprets the image and LSTM translate the image context into sentences.</p>
							<p>Subsequent "show, attend and tell" introduces a attention vector to tell the LSTM where to look at within the CNN output. This attention vector itself is generated from fully connected neural net and is trainined together with the system. Evaluation results suggest small advantage beyond original "Show and Tell" algorithm</p>
							<img src="img/imagecaptioning/googlresult.jpg" class="img-responsive" alt="Show and Tell Result"/>
							<div class="col-lg-8">
								<table class="table table-bordered table-hover table-responsive"> 
									<tbody> 
										<tr> 
											<th scope="row" rowspan="2">Accuracy</th>
											<td>Show and Tell - MSCOCO dataset: BLUE-4: 24.6, METEOR: 23.7</td>
										</tr>
										<tr>
											<td>Show, Attend and Tell - MSCOCO dataset: BLUE-4: 25.0, METEOR: 23.04</td>
										</tr>
										<tr> 
											<th scope="row">Architecture</th>
											<td>
												<ul>
													<li>Show and Tell concatenate LSTM network after GoogleNet CNN. It directly uses CNN's output as the input to LSTM.</li>
													<li><span>Show, Attend and Tell connect a fully connected network to the output of CNN, which generate a attention vector. The element wise product of the attention vector and CNN's output is then feed into LSTM network.</span>
														<img src="img/imagecaptioning/showattendandtell.png" class="img-responsive" alt="Show, Attend and Tell"/></li>
												</ul>
											</td>
										</tr>
										<tr> 
											<th scope="row">Advantage</th>
											<td>
												<ul>
													<li>Higher accuracy: these are first deep learning based image captioning algorithms that achieved great performance in image captioning at scale.</li>
												</ul>
											</td>
										</tr>
									</tbody>
								</table>
							</div>
							<div class="col-lg-4"></div>
						</div>
						<div class="col-lg-12 row">
							<p>Show and Tell: <a href="https://github.com/jazzsaxmafia/show_and_tell.tensorflow">Github</a></p>
							<p>Show, Attend and Tell: <a href="https://github.com/jazzsaxmafia/show_attend_and_tell.tensorflow">Github</a></p>
						</div>
						<hr/>
					</div>	
					<div id="textrank" class="row">
						<div class="col-lg-12 row">
							<h1>TextRank - Article Summarization & Keyword Extraction</h1>
							<p class="lead">Publication: <a href="http://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf">TextRank: Bringing Order into Texts</a></p>
							<p>TextRank applied <a href="https://en.wikipedia.org/wiki/PageRank">PageRank</a>-style graph ranking algorithm on natural language articles. Through tokenization of individual words as vertex and using co-occurance as unweighted connection, text rank graph can produce a list of keywords from a passage.</p>
							<p>The author also achieved good performance in article summarization if the algorithm uses whole sentence as vertex and use "sentence similarity" as weighted connection.</p>
							<div class="col-lg-8">
								<table class="table table-bordered table-hover table-responsive"> 
									<tbody> 
										<tr> 
											<th scope="row">Accuracy</th>
											<td>TextRank outperforms many previous approaches in this area (e.g. Hulth, 2003)</td>
										</tr> 
										<tr> 
											<th scope="row">Architecture</th>
											<td>
												<ul>
													<li>The algorithm computes the exact same way as PageRank/HITS rank.</li>
												</ul>
											</td>
										</tr>
										<tr> 
											<th scope="row">Advantage</th>
											<td>
												<ul>
													<li>It is a text processing implementation of the already well known “pagerank” and “HITS” algorithms.</li>
													<li>The model is language independent, with little external knowledge required.</li>
												</ul>
											</td>
										</tr>
										<tr> 
											<th scope="row">Disadvantage</th>
											<td>
												<ul>
													<li>It can only ensure high quality for small sets of documents where text graph can be accurately constructed using proposed similarity measures.</li>
												</ul>
											</td>
										</tr>
									</tbody>
								</table>
							</div>
							<div class="col-lg-4"></div>
						</div>
						<div class="col-lg-12 row">
							<p>JS Sample Code: <a href="https://github.com/dpressel/textrank-js">JS TextRank</a></p>
							<p>Python Sample Code: <a href="https://github.com/davidadamojr/TextRank">Python TextRank</a></p>
							<p>And many more on Github</p>
							
						</div>
						<hr/>
						
					</div>	
					<div id="syntaxnet" class="row">
						<div class="col-lg-12 row">
							<h1>SyntaxNet: Neural Models of Syntax</h1>
							<p class="lead">Google's Open-Source Model & Code: <a href="https://github.com/tensorflow/models/tree/master/research/syntaxnet">SyntaxNet: Neural Models of Syntax</a></p>
							<p>Part of speech (POS) tagging aims at parsing the dependency structure of a sentence to understand which word is root, action and objectives. SyntaxNet is a Google open-sourced neural network solution achieving state-of-art accuracy in POS challenges.</p>
							<img src="img/syntaxnet/ff_nn_schematic.png" class="img-responsive" alt="DependencyParsing"/>
							<p>As illustrated in the above picture, the model concatenates feature embeddings, run through hidden layer, and output the softmax probability for the next optimal action to take on the sequence graph.</p>
							<p>The mathematics in <a href="https://arxiv.org/abs/1603.06042">Globally Normalized Transition-Based Neural Networks</a> is critical in training a high accuracy syntax net.</p>
							<p>In summary, the paper proves</p>
							<p>
								<ul>
									<li>Computing global normalization loss function is strictly better (more expressive) than local normalization loss function. 
										<ul>
											<li>Global normalization: how likely is this sequence (of tagging), out of all possible sequences, the golden sequence?</li>
											<li>Local normalization: how likely is this next action (tagging), out of all possible next actions, leads the current sequence to the golden sequence?</li>
										</ul>
									</li>
									<li>
										An efficient way to compute the global normalization function. Specifically, the computation can be optimized through beam searching only the golden path and early termination - perform backpropregation as soon as the system prediction deviates from the golden path, and stops there.
									</li>
								</ul>
							</p>
						</div>
						<hr/>
					</div>	
					<div id="gan" class="row">
						<div class="col-lg-12 row">
							<h1>GAN: Generative Adversarial Network</h1>
							<p class="lead">Publication: <a href="https://arxiv.org/abs/1406.2661">Generative Adversarial Networks</a></p>
							<p>Generative models are useful for building AI that can self-compose images, music and other works. Building a generative model is challenging because it is hard to define what is the best output (training target), and find a working cost function.</p>
							<p>GAN introduces a new paradigm of training a generative model, in the following way:</p>
							<p>
								<ul>
									<li>
									    Build a generative model neural network 
									</li>
									<li>
										Build a discriminator network that tries to tell if its input (e.g. an image) is artificially generated or real.
									</li>
								</ul>
							</p>
							<p>In the paper, the author used fully connected layers for both networks and demonstrated good performance in generating realistic looking images.</p>
							<p>The second network can be trained via backpropagation because we know for each image if it is generated or not. During the training of first netwrok, or generative network, we can lock the second network and use backpropagation to tell the first network to go into the direction of making the second network say it is more real than generated. Conceptually, the second network is providing advice for the first network on how to make its output more realistic and thus named as Adversarial network.</p>
							<p>The following image demonstrated this process visually</p>
							<img src="img/gan/gan.png" class="img-responsive" alt="GAN"/>
							<p>
							<ul>
									<li>
									    Back dot: real output
									</li>
									<li>
										Grean line: generative network output
									</li>
									<li>
										Blue line: discriminator network output (0.5 means it believe the work has equal probability to be real or generated)
									</li>
								</ul>
							</p>
							<p>Although as Karpathy noted on his <a href="http://cs.stanford.edu/people/karpathy/gan/">experiment</a>, "things are not quite as nice in practice".</p>
						</div>
						<hr/>
					</div>
					<div id="vae" class="row">
						<div class="col-lg-12 row">
							<h1>Variational Autoencoder</h1>
							<p class="lead">Publication: <a href="https://arxiv.org/abs/1312.6114">Original VAE paper (2013)</a></p>
							<p class="lead">Publication: <a href="https://arxiv.org/abs/1606.05579">Disentangled VAE's (DeepMind 2016)</a></p>
							<p class="lead">Video: <a href="https://www.youtube.com/watch?v=9zKuYvjFFS8">Variational Autoencoders Walk-through</a></p>
							<p>Variational autoencoder differs from a traditional neural network autoencoder by merging statistical modeling techniques with deep learning</p>
							<p>Specifically, it is special in that:</p>
							<p>
								<ul>
									<li>
									    It tries to build encoded latent vector as a Gaussian probability distribution of mean and variance (different mean and variance for each encoding vector dimension).
									</li>
									<li>
										The latent vector forms a continuous space, tuning latent vector continuously forms continuous output representations. With disentangled VAE, the latent vector can even minimizes their correlations, and become more orthogonal to one another. This is perhaps the <b>best property</b> a traditional autoencoder lacks.
									</li>
									<li>
										During reconstruction stage, a stochastic operation (random sample from Gaussian) is performed to first generate the latent vector. This sample can be considered as a constant in the backpropagation stage
									</li>
									<li>
										The cost function include similarity towards the target (same as traditional autoencoder) and a KL divergence that pushes the latent vector converge to Gausian distribution.
									</li>
								</ul>
							</p>
							<p>The following image demonstrated VAE network</p>
							<img src="img/vae/vae.jpg" class="img-responsive" alt="GAN"/>
							<p>Kevin provides <a href="http://kvfrans.com/variational-autoencoders-explained/">a more detailed explanation with codes</a>, coming from both deep learning and statistician perspectives.
						</div>
						<hr/>
					</div>
				</div>
				<div class="col-md-1"></div>
			</div>
        
			<div class="container footer">
				<hr/>

				<!-- Footer -->
				<footer>
					<div class="row">
						<div class="col-sm-6 col-lg-12">
							<p>Want to report inaccurate description or contribute to the model documentation on the site? Find us on <a href="https://github.com/lexionbear/mlmodels/tree/master">Github</a></p>
							<!--<p>Copyright &copy; lexionbear.github.io 2016</p>-->
						</div>
					</div>
				</footer>

			</div>
		</div>
        <!-- /#page-content-wrapper -->

    </div>
    <!-- /#wrapper -->
	
    <!-- jQuery -->
    <script src="js/jquery.js"></script>

    <!-- Bootstrap Core JavaScript -->
    <script src="js/bootstrap.min.js"></script>

    <!-- Custom Script -->
    <script>

    </script>
	
</body>

</html>
